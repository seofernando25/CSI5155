\section{Methodology}

The experiment consists of two parts: establishing a classical baseline using an SVM with Fisher Vector features, and experimentally inducing a model-wise double descent curve by training a series of scalable CNNs.

\subsection{Dataset and Preprocessing}

The CIFAR-10 dataset consists of 60,000 $32\times32$ color images across 10 classes. The standard partitioning is used: a 50,000-image training set and a 10,000-image test set. The training set is further split into 40,000 training and 10,000 validation images. The official test set is held out for final evaluation.

To amplify the double descent effect \cite{nakkiran2019deep}, 20\% label noise is introduced to the 40,000-image training set. This is achieved by permanently replacing the true label of 8,000 randomly selected images with a uniformly random incorrect label. The validation and test sets remain "clean." All models are trained on the noisy training set and evaluated on the clean sets.

\subsection{Classical Baseline}

The classical baseline utilizes a Linear Support Vector Classifier (LinearSVC) with Fisher Vector (FV) features. Dense $8 \times 8$ patches (stride 4) are extracted and reduced to $D = 24$ dimensions using PCA with whitening (retaining $\approx$96\% variance). A Gaussian Mixture Model (GMM) with $K = 64$ components fits the reduced patches, yielding a $3{,}072$-dimensional Fisher Vector per image. These FVs are standardized and used to train an $L_2$-regularized LinearSVC. The cost parameter $C$ is tuned on the validation set.

\subsection{CNN Architecture}

To answer RQ2, a model-wise double descent curve is generated using a $\text{ScaledCNN}(k)$ architecture where layer widths scale by factor $k$, as visualized in \autoref{fig:scaledcnn}.

\begin{figure}[ht]
\centering
\begin{tikzpicture}[
    node distance=0.8cm,
    conv/.style={draw=black, align=center},
    norm/.style={ draw=black, align=center},
    act/.style={ draw=black, align=center},
    pool/.style={ draw=black, align=center},
    linear/.style={ draw=black, align=center},
    arrow/.style={->, thick, >=stealth},
]
    % Block 1
    \node[conv] (conv1) {Conv2d\\$3 \to 16k$\\kernel\_size=3\\padding=1};
    \node[norm, right=of conv1] (bn1) {BatchNorm};
    \node[act, right=of bn1] (relu1) {ReLU};
    \node[pool, right=of relu1] (pool1) {MaxPool};
    
    % Block 2
    \node[conv, below=1.5cm of conv1] (conv2) {Conv2d\\$16k \to 32k$\\kernel\_size=3\\padding=1};
    \node[norm, right=of conv2] (bn2) {BatchNorm};
    \node[act, right=of bn2] (relu2) {ReLU};
    \node[pool, right=of relu2] (pool2) {MaxPool};
    
    % Block 3
    \node[conv, below=1.5cm of conv2] (conv3) {Conv2d\\$32k \to 64k$\\kernel\_size=3\\padding=1};
    \node[norm, right=of conv3] (bn3) {BatchNorm};
    \node[act, right=of bn3] (relu3) {ReLU};
    \node[pool, right=of relu3] (pool3) {AdaptiveAvgPool};
    
    % Final layers
    \node[below=1.5cm of conv3, minimum width=1.2cm] (flat) {Flatten};
    \node[linear, right=of flat] (linear) {Linear\\$64k \to 10$};
    
    % Arrows within blocks
    \draw[arrow] (conv1) -- (bn1);
    \draw[arrow] (bn1) -- (relu1);
    \draw[arrow] (relu1) -- (pool1);
    
    \draw[arrow] (conv2) -- (bn2);
    \draw[arrow] (bn2) -- (relu2);
    \draw[arrow] (relu2) -- (pool2);
    
    \draw[arrow] (conv3) -- (bn3);
    \draw[arrow] (bn3) -- (relu3);
    \draw[arrow] (relu3) -- (pool3);
    
    \draw[arrow] (pool3) -- (flat);
    \draw[arrow] (flat) -- (linear);
    
    % Vertical connections between blocks
    \draw[arrow] (pool1) -- (conv2);
    \draw[arrow] (pool2) -- (conv3);
    
    % Input
    \node[left=0.5cm of conv1, align=center] (input) {Input\\$32 \times 32 \times 3$};
    \draw[arrow] (input) -- (conv1);
    
    % Output
    \node[right=0.5cm of linear, align=center] (output) {Output\\10 classes};
    \draw[arrow] (linear) -- (output);
\end{tikzpicture}
\caption{Architecture of $\text{ScaledCNN}(k)$.}
\label{fig:scaledcnn}
\end{figure}

\subsubsection{Training Configuration}

To isolate model capacity ($k$) as the independent variable, all other hyperparameters are fixed (Table \ref{tab:hyperparams}). This setup adopts the standard protocol for replicating double descent on CIFAR-10 with label noise \cite{nakkiran2019deep}.

\begin{table}[H]
    \centering
    \caption{Training hyperparameters for ScaledCNN(k).}
    \label{tab:hyperparams}
    \begin{tabular}{ll}
        \toprule
        Hyperparameter & Value \\
        \midrule
        Optimizer & Adam \\
        Learning Rate & $1 \times 10^{-4}$ (constant) \\
        Weight Decay & $0.0$ \\
        Epochs & 500 \\
        \bottomrule
    \end{tabular}
\end{table}

This configuration ensures sufficient time for models to fit the noisy training data, which is required for the "second descent."

\subsubsection{Training and evaluation}

Models with widths $k \in \{1, 2, 4, 8, 16, 32, 64\}$ were trained from scratch on the noisy training set. Final performance was evaluated on the clean test set to plot Test Accuracy vs. Model Capacity.

\subsection{Analysis}

\subsubsection{Statistical Significance}

As the t-test is invalid for comparing correlated classifiers on the same test set (which were trained only once) \cite{compare_two_classifiers}, the non-parametric McNemar's Test \cite{demsar2006statistical} is used to determine if the observed differences are statistically significant ($p < 0.05$). I computed the contingency tables and p-values using the MLxtend library \cite{raschkas_2018_mlxtend}.

\subsubsection{Error Analysis}

Confusion matrices ($10\times10$) are generated for the SVM baseline, the critically-parameterized $\text{ScaledCNN}(k=4)$, and the over-parameterized $\text{ScaledCNN}(k=64)$. These will be analyzed qualitatively to identify changes in inter-class confusion.