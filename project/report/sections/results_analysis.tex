\section{Results and Analysis}

\subsection{Classical Baseline Performance}

The SVM + FV baseline achieves a test accuracy of 53.64\% (\autoref{tab:svm_performance}). The confusion matrix (\autoref{fig:svm_confusion}) shows significant errors on certain inter-classes trucks/automobiles and cats/dogs, which suggests the FV features are insufficient for separating visually similar categories.

\begin{table}[H]
    \centering
    \caption{Test-set performance of the SVM + FV baseline.}
    \label{tab:svm_performance}
    \begin{tabular}{lccc}
        \toprule
        Model & Accuracy (\%) & Macro F1 & Weighted F1 \\
        \midrule
        SVM + FV & 53.64 & 0.53 & 0.53 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/SVM__test__confusion_matrix.pdf}
    \caption{Normalized confusion matrix for the SVM + FV baseline on the CIFAR-10 test set.}
    \label{fig:svm_confusion}
\end{figure}

\subsection{Epoch-Wise and Model-Wise Double Descent}

The $\text{ScaledCNN}(k)$ models exhibit both epoch-wise and model-wise double descent.

Epoch-wise double descent appears in wider models ($k \ge 8$, \autoref{fig:scaledcnn_error_curves}). For $k=16$, validation error initially decreases, spikes to $\approx 0.70$ as the model interpolates the noisy training set (train error $\to 0$ near epoch 100), and subsequently recovers to $\approx 0.29$ as the optimizer locates a better-generalizing solution \cite{nakkiran2019deep}.

Model-wise double descent is confirmed by test set performance (\autoref{tab:scaledcnn_test_performance} and \autoref{fig:capacity_curve}). Test accuracy drops from 69.28\% ($k=1$) to a trough of 67.84\% at the interpolation threshold ($k=4$). Beyond this "critically-parameterized" point, accuracy recovers, climbing to a peak of 76.55\% at $k=64$ in the "second descent."

\begin{table}[H]
    \centering
    \caption{Test-set performance of $\text{ScaledCNN}(k)$ on CIFAR-10.}
    \label{tab:scaledcnn_test_performance}
    \begin{tabular}{cccc}
        \toprule
        Width $k$ & Trainable Params & Accuracy (\%) & Macro F1 \\
        \midrule
        1  & $2.4\times 10^{4}$  & 69.28 & 0.69 \\
        2  & $9.5\times 10^{4}$  & 67.87 & 0.68 \\
        4  & $3.7\times 10^{5}$  & 67.84 & 0.67 \\
        8  & $1.5\times 10^{6}$  & 68.43 & 0.66 \\
        16 & $5.9\times 10^{6}$  & 71.02 & 0.71 \\
        32 & $2.4\times 10^{7}$  & 74.29 & 0.74 \\
        64 & $9.4\times 10^{7}$  & 76.55 & 0.77 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/scaledcnn_capacity_vs_performance.pdf}
    \caption{Test accuracy and macro F1 vs. trainable parameters (log scale) for $\text{ScaledCNN}(k)$.}
    \label{fig:capacity_curve}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/scaledcnn_k4_error_curve.pdf}
    \includegraphics[width=0.45\textwidth]{figures/scaledcnn_k16_error_curve.pdf}
    \caption{Epoch-wise train/validation error for $k=4$ (left) and $k=16$ (right). The $k=16$ model shows a clear "dip and recovery" after interpolating the noisy training data.}
    \label{fig:scaledcnn_error_curves}
\end{figure}

\subsection{Statistical and Error Analysis (RQ3)}

\subsubsection{Statistical Significance}

McNemar's test \cite{demsar2006statistical} confirms that the over-parameterized $\text{ScaledCNN}(k=64)$ outperforms the SVM baseline ($p \approx 0$, \autoref{tab:mcnemar_results}).

The test also verifies that the initial accuracy drop from $k=1$ to $k=2$ is significant ($p = 0.0037$), while the difference between $k=2$ and $k=4$ is not ($p = 0.967$), statistically identifying the trough. The recovery in the over-parameterized regime is robust: improvements from $k=8 \to 16$, $k=16 \to 32$, and $k=32 \to 64$ are all significant ($p < 0.001$).

\begin{table}[H]
    \centering
    \caption{McNemar's test results for key model comparisons on the 10,000-sample test set.}
    \label{tab:mcnemar_results}
    \begin{tabular}{llcc}
        \toprule
        Model A & Model B & p-value & Significant \\
        \midrule
        SVM & $\text{ScaledCNN}(k=1)$ & $6.56 \times 10^{-167}$ & Yes \\
        SVM & $\text{ScaledCNN}(k=64)$ & $2.66 \times 10^{-314}$ & Yes \\
        $\text{ScaledCNN}(k=1)$ & $\text{ScaledCNN}(k=2)$ & $3.69 \times 10^{-3}$ & Yes \\
        $\text{ScaledCNN}(k=2)$ & $\text{ScaledCNN}(k=4)$ & $0.967$ & No \\
        $\text{ScaledCNN}(k=4)$ & $\text{ScaledCNN}(k=8)$ & $0.206$ & No \\
        $\text{ScaledCNN}(k=8)$ & $\text{ScaledCNN}(k=16)$ & $1.24 \times 10^{-7}$ & Yes \\
        $\text{ScaledCNN}(k=16)$ & $\text{ScaledCNN}(k=32)$ & $2.34 \times 10^{-12}$ & Yes \\
        $\text{ScaledCNN}(k=32)$ & $\text{ScaledCNN}(k=64)$ & $7.92 \times 10^{-10}$ & Yes \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Qualitative Error Analysis}

Comparison of confusion matrices (\autoref{fig:svm_confusion}, \autoref{fig:scaledcnn_confusions}) confirms that over-parameterization yields superior inter-class discrimination. The SVM baseline confuses `cat' $\to$ `dog' 19.1\% of the time and still mistakes `bird' for `airplane' 10.3\%. The critically-parameterized $\text{ScaledCNN}(k=4)$ inherits the same weaknesses, misclassifying `bird' $\to$ `airplane' 18.5\% of the time despite modest gains elsewhere.

The over-parameterized $\text{ScaledCNN}(k=64)$ resolves these ambiguities. It reduces `bird' $\to$ `airplane' confusion to 6.2\% and lowers `cat' $\to$ `dog' to 10.6\%, supporting the hypothesis that the implicit regularization of the "second descent" produces more discriminative features.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/scaledcnn_k4_test_confusion_matrix.pdf}
    \includegraphics[width=0.45\textwidth]{figures/scaledcnn_k64_test_confusion_matrix.pdf}
    \caption{Normalized confusion matrices for the "peak error" model $\text{ScaledCNN}(k=4)$ (left) and the "second descent" model $\text{ScaledCNN}(k=64)$ (right).}
    \label{fig:scaledcnn_confusions}
\end{figure}
