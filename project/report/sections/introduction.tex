\section{Introduction}

Historically, computer vision relied on feature engineering, where handcrafted representations (e.g., SIFT, Fisher Vectors) were processed by classical models like Support Vector Machines (SVMs). This approach followed the bias-variance trade-off, aiming for a medium-complexity model to minimize both underfitting and overfitting. This project establishes an SVM with Fisher Vectors (FV) as a baseline for this classical regime.

Deep learning models, such as AlexNet \cite{krizhevsky2012imagenet}, challenged this paradigm by learning hierarchical features from data. However, modern networks are often over-parameterized, possessing more trainable parameters than samples \cite{zhang2017understanding}. While the classical bias-variance trade-off predicts overfitting for such models, recent work has shown that deep networks can memorize random noise yet still generalize to real data \cite{zhang2017understanding}.

The "double descent" phenomenon \cite{belkin2019reconciling} addresses this discrepancy by extending the classical test error curve into three regimes:

\begin{itemize}
    \item \textbf{Underparameterized:} Test error decreases as capacity increases (classical descent).
    \item \textbf{Interpolation threshold:} Test error peaks as the model effectively fits the training data.
    \item \textbf{Overparameterized:} Test error decreases again as capacity increases further ("second descent") \cite{nakkiran2019deep}.
\end{itemize}

In the over-parameterized regime, stochastic gradient optimizers act as implicit regularizers, biasing optimization toward simpler, better-generalizing solutions \cite{zhang2017understanding}. This project investigates this generalization spectrum on the CIFAR-10 dataset \cite{krizhevsky2009learning}, guided by three research questions:

\begin{itemize}
    \item \textbf{RQ1:} How does the performance of a classical baseline (SVM + FV) compare to an over-parameterized CNN?
    \item \textbf{RQ2:} Can a model-wise double descent curve be induced by scaling CNN width \cite{nakkiran2019deep}, and where does the interpolation peak occur?
    \item \textbf{RQ3:} What are the quantitative generalization differences between classical, critically-parameterized, and over-parameterized models?
\end{itemize}

I hypothesize that: (1) the over-parameterized CNN will significantly outperform the SVM baseline; (2) scaling model width will reproduce the double descent curve; and (3) over-parameterized models will demonstrate superior inter-class discrimination compared to simpler models.